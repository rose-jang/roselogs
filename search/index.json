[{"content":"Introduction This project simulates a simplified Security Operations Centre (SOC) environment within a small enterprise network. It uses open-source tools including Wazuh, Snort, and Sysmon — to demonstrate how cyberattacks can be detected and monitored in real-time.\nIn this post, it focuses on setting up the internal network and essential security tools. Real-world attack scenarios will be excecuted to observe how the SOC components respond in the next post.\nBy emulating a realistic attack-and-defense scenario, this project aims to deepen understanding of SOC operations and network security monitoring.\nScenario Overview The SOC home lab consists of three virtual machines running on VirtualBox, each serving a different role:\nVictim: A Windows machine with Sysmon and the Wazuh agent installed to log and send system activity data. SOC Server: An Ubuntu machine running Wazuh (as the SIEM) and Snort (as the IDS) to collect, analyze, and alert on suspicious behaviour. Attacker: A Kali Linux machine used to simulate offensive actions, including port scanning and credential dumping using tools like Nmap and Mimikatz. This setup assumes that the attacker already has gained access to the internal network. This reflects post-compromise behaviour for realistic threat emulation.\nEnvironment Setup Network configuration All virtual machines in this lab are connected using a Host-only network adapter to ensure isolation from the internet, allowing safe and controlled testing. Each machine is configured with a static IP address to simplify traffic analysis and eliminate ambiguity during monitoring.\nStatic IP assignments:\nUbuntu (SOC Server): 192.168.56.10 Windows (Victim): 192.168.56.20 Kali (Attacker): 192.168.56.30 Tooling \u0026amp; Configuration Ubuntu - SOC Server SIEM: Wazuh Installation\nInstall Wazuh All-in-One package (from Wazuh Quickstart), which includes: Wazuh Manager Filebeat Elasticsearch Kibana Verify server dashboard access: Server IP: 192.168.56.10, Port 5601 URL: http://[serverIP]:5601 Agent Deployment\nAdd an agent from Wazuh dashboard Troubleshooting for agent deployment on dashboard a. Run the agent manager in CLI: sudo /var/ossec/bin/manage_agents b. Follow prompts to create an agent c. Extract the authentication key of the agent d. Enter the key into the Wazuh Agent software on Windows machine e. Restart both the agent and server Confirm the Windows machine appears in the dashboard IDS/IPD: Snort Quick Note: Difference between IDS/IPS?\nIDS (Intrusion Detection System) alerts on suspicious activities based on defined rules. IPS (Intrusion Prevention System) acts a step further by actively responding to malicious traffics. Installation\nInstall in CLI: sudo apt-get install snort -y Define the network range to monitor during installation (e.g., 192.168.56.0/24) Test Snort: sudo snort -v Configuration\nSnort file path Configs: /etc/snort Logs: /var/log/snort Rules: /etc/snort/rules Rule configuration Add rules into the file \u0026gt; /etc/snort/rules/local.rules Alert rule configuration structure: Header - Body - Actions alert [protocol] [source] [port] → [destination] [port] (body… msg:”~~detected”;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Detect open port scanning alert tcp any any -\u0026gt; $HOME_NET 1:1024 ( flags: S; msg: \u0026#34;PORT SCAN detected\u0026#34;; threshold: type both, track by_src, count 10, seconds 3; sid: 1001; rev: 1; ) # Detect Mimikatz alert tcp any any -\u0026gt; $HOME_NET any ( msg: \u0026#34;Mimikatz detected\u0026#34;; content: \u0026#34;mimikatz\u0026#34;; nocase; sed: 1002; rev: 1; ) Detect open port scanning flags: S; # Flags for TCP SYN threshold: # Triggers when 10 SYN packets in 3 seconds Detect Mimikatz content: # Detects a string “mimikatz”, case insensitive Windows - Victim Sysmon: Activity logging Installation\nDownload Sysmon file on Microsoft official site Use the configuration file provided by Wazuh (save as XML file) Install via PowerShell: ./sysmon64.exe -i sysconfig.xml Wazuh Agent Installation\nInstall by using command retrieved from Wazuh dashboard Or insert authentication key from Agent management \u0026gt; Refer to the Agent deployment in SIEM: Wazuh section above Confirm logs and events appearing on the Wazuh dashboard Conclusion This post covered the setup of a simplified SOC lab using tools such as Wazuh, Snort, and Sysmon across isolated virtual machines. By isolating the network and configuring static IPs, we created a controlled space for observing attacker behaviors and system responses. With all components successfully deployed and communicating, the foundation is now ready for simulating and detecting real-world cyberattacks.\nThe simulation using penetration tools on Kali will be demonstrated in the next blog post.\n","date":"2025-06-08T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/soc-environment-home-lab-setting-up/socHomeLab-banner_hu_5c4fdc2e352d11e2.png","permalink":"https://rose-jang.github.io/roselogs/p/soc-environment-home-lab-setting-up/","title":"SOC Environment Home Lab - Setting Up"},{"content":"Cyber Kill Chain To effectively defend a network, we must learn to think like an attacker. By understanding how cyber attacks unfold, we can proactively secure our systems and prevent damage before it happens. One of the key steps in that process is learning the Cyber Kill Chain.\nWhat is it? The Cyber Kill Chain is a foundational framework in cybersecurity that outlines the typical stages of a cyber attack—from initial planning to achieving the attacker’s end goal. Developed by Lockheed Martin, this model helps security teams understand the attack lifecycle in depth. By identifying and interrupting attacks at various stages, defenders can significantly reduce the likelihood of a successful breach.\n7 Stages 1-Reconnaissance Planning phase, attackers gather information about their target\nPassive: Collect data without direct interaction (e.g., archived websites, social media, public records) Active: Direct engagement (e.g., scanning ports, pining servers**)** Defender’s role:\nAvoid publicly exposed information Patch known vulnerability in an early stage Monitor suspicious network activity (scanning or probing) 2-Weaponization Prepare a payload (bundle of malicious codes) to exploit using the information collected\nDefender’s role:\nUse the latest threat intelligence Configure detection rules for common exploits 3-Delivery Expose the payload to lure victims (vis phishing emails, malicious websites/attachments, USBs, …)\nDefender’s role:\nTest suspicious files in sandbox Educate users on phishing attacks Monitor for unusual behaviour 4-Exploitation Payload executed once vulnerability is triggered\nDefender’s role:\nUser training to avoid clicking suspicious links/files EDR (Endpoint Detection \u0026amp; Response) monitoring Continuous monitoring for unusual behaviour 5-Installation Malware installation (backdoor, web shell) on the compromised system\nDefender’s role:\nMonitor OS settings change Block unauthorized system tasks or services Threat hunting to detect uncommon installation 6-Command\u0026amp;Control (C2) The malware contacts a remote server that manipulates the infected machine\nDefender’s role:\nAnalyze outbound traffic Block known malicious IPs/domains Isolate infected systems 7-Actions on Objectives Accomplish initial goals by stealing data, damage systems, or ransomware, …\nDefender’s role:\nRestrict network access from external Implement DLP to prevent data leakage Lock down sensitive resources Conclusion We explored the stages of the Cyber Kill Chain that offers valuable insight into how attackers operate, and how defenders respond at each phase. Understanding this model is a crucial step toward building a proactive and effective cybersecurity strategy.\n","date":"2025-05-16T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/understanding-the-cyber-kill-chain/cyberkillchain-banner_hu_4f2f424def6fe994.png","permalink":"https://rose-jang.github.io/roselogs/p/understanding-the-cyber-kill-chain/","title":"Understanding the Cyber Kill Chain"},{"content":"Project Overview In this project, I built a simple note-taking web app that features a clean and minimalistic design, providing a user-friendly interface for taking quick notes. To make the app accessible to users on the web, I hosted it using AWS (Amazon Web Services), specifically using Amazon S3 for static website hosting and Route 53 for domain management. By deploying the app on AWS, we can ensure that it is scalable, secure, and easily accessible.\nPrerequisites AWS account (preferably IAM admin user) Static website files Domain name Used AWS Services To maximize the functionality of the web application, several AWS services were utilized in this project. These services enhance security, availability, and user accessibility on the front end while ensuring efficient management and cost-effectiveness on the back end.\nS3 (Simple Storage Service) Route 53 (for Domain Name Service) CloudFront (as Content Network Delivery) AWS Certificate Manager (to activate HTTPS) Content management on S3 Amazon S3 (Simple Storage Service) is a highly scalable object storage service that allows us to store and manage virtually unlimited amounts of data. It is particularly useful for hosting static websites, including HTML, CSS, JS, images, and other files.\nSetting Up Bucket Create a bucket Create a general bucket with a globally unique name Leave other settings as default Upload files After the bucket is created, simply drag \u0026amp; drop files into the bucket Configuration After uploading the website files, we need to configure the bucket for static website hosting.\nProperties Enable Static website hosting Hosting type: Static website Index document: index.html (landing page) Keep other settings as default Permissions Turn off Block all public access, which will allow the bucket to be publicly accessible Edit Bucket policy as below, in JSON format (Use your bucket ARN as Resource) 1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::note.rosespace.me/*\u0026#34; } ] } Verify Accessibility In Properties, scroll down to the Bucket website endpoint The endpoint displays the bucket objects as a static website Domain name setup with Route 53 Now that the website is hosted on S3, we can set up a custom domain using AWS Route 53 to make the site more user-friendly and accessible.\nCreate a DNS Record Access to Route 53, and select your domain Under Records, click Create record Configure the CNAME record (subdomain) Record name: Name the website as you want Record type: CNAME (Canonical Name) Value: Enter the website endpoint retrieved from the bucket Check Accessibility Once the DNS record is created, try accessing the domain If everything is configured correctly, the website should be up and running without issues CloutFront To further enhance the speed, security, and global accessibility of the website, let\u0026rsquo;s deploy a CloudFront distribution and issue a a public SSL/TLS certifiacate via ACM, AWS Certificate Manager.\nInitialize CloudFront Distribution Create a distribution Navigate to CloudFront and create a new distribution Choose the S3 bucket (static website endpoint) as the origin domain Ensure the bucket name and custom domain name match for routing Distribution configuration Configure further for proper access Viewer: Redirect HTTP to HTTPS Price class: Select desired regions Alternate domain name: Add the domain name configured previously Request a certificate If you don\u0026rsquo;t have one yet, request a new public certificate Fully qualified domain name: Enter the FQDN Validation method: DNS validation - Confirm the certificate status and return to the distribution setup - Select the new certificate from the list - Complete the distribution creation process Edit Route 53 CNAME record Once the distrubution is deployed and enabled: Go to Route 53 and edit the existing CNAME record Replace the value with the distribution domain name Verify Website Accessibility If everything is configured correctly and running, you can now access the secure website. The website is now: Globally distributed via CloudFront Securely accessible with HTTPS via the ACM-issued cerficicate Properly routed through the custom domain name Conclusion By integrating AWS S3, Route 53, CloudFront, and ACM, we successfully hosted a static web application with a robust and modern infrastructure. The website is not only highly available and securely accessible via HTTPS, but also globally distributed with low latency and reachable through a custom domain. This setup ensures a smooth and reliable user experience while taking full advantage of AWS’s scalability, performance, and cost-effectiveness.\n","date":"2025-04-04T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/web-app-hosting-on-aws-s3/cloud-banner_hu_febc072823cee19f.png","permalink":"https://rose-jang.github.io/roselogs/p/web-app-hosting-on-aws-s3/","title":"Web App hosting on AWS S3"},{"content":"What is Kubernetes? Kubernetes is an open-source platform designed to manage containerized applications at scale. It automates the deployment, scaling, and management of application containers across clusters of machines.\nKubernetes is capable of running thousands of replicas of an application simultaneously, ensuring high availability and reliability. Each replica (an instance of an application) can be accompanied by additional containers, such as sidecars, which serve as \u0026ldquo;managers\u0026rdquo; performing essential tasks such as logging, proxying, or monitoring.\nA key concept in Kubernetes is the shared lifecycle of containers within a pod, ensuring that all components of the application work together seamlessly. This design simplifies the deployment and management of applications, especially in large-scale environments where consistency and reliability are paramount.\nFundamental Components Containers Packaging all dependencies, code, and runtime of exact versions\nContainers ensures smooth operation across both development and production environments. Since modern applications require frequent updates and changes, containers make it easier for developers to implement and distribute updates without disrupting services. Dev teams can efficiently share and update applications, while Ops teams can maintain scalable and reliable deployments. Pod The smallest deployable unit in Kubernetes\nA pod encapsulates one or more tightly coupled containers. Typically a pot hosts a single main application container, but it can include additional containers (sidecars) that assist the main application. Containers in a same pod share: Storage volumes Lifecycle management All containers in a pod start/stop/restart altogether, ensuring they function as a cohesive unit. Network resources (same IP address / port space) They communicate internally via localhost hostname with their designated ports. ReplicaSet Ensures desired pod availability and redundancy\nA ReplicaSet is responsible for maintaining a stable set of identical pods, using a template. Maintains the desired number of pods running at all time, automatically replacing failed pods. Purpose: High Availability, redundancy, and scalability. Deployment Manages ReplicaSets and facilitates rolling updates\nDeployment is a higher-level abstraction that manages ReplicaSets Helps automate the scaling and rollout process while minimizing manual intervention. Services Configuration for network and load balancing\nKubernetes pods are ephemeral, they can be created / destroyed dynamically. This means their IP addresses are not assigned statically. Services provide a stable endpoint to resolve this issue for accessing pods. Types of Services:\nCluster IP (default type) Exposes the service internally within the cluster. Allows communication between pods in the same cluster. Assigns an internal IP address and DNS name, unreachable from the external. Service detects the correct pod based on the given selector name Service: spec.selector.app = Pod: metadata.labels.app NodePort Exposes the service on a static port on each node\u0026rsquo;s IP. Enabling the service accessible from outside the cluster using :. LoadBalancer Exposes the service externally via a cloud-based load balancer. Suitable for production environments requiring public access and traffic distribution. Summary This post covered Kubernetes\u0026rsquo; core building units: Containers, Pods, ReplicaSets, Deployments, and Services. These components work together to provide a scalable, resilient system for managing containerized applications.\n","date":"2025-03-27T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/kubernetes-basics/k8s-banner_hu_8fd1cc0a03c9c623.png","permalink":"https://rose-jang.github.io/roselogs/p/kubernetes-basics/","title":"Kubernetes Basics"},{"content":"I journaled about the basics of Docker in this post. Starting with a brief overview of what docker is and concept of containerization. I delve into how containerization differs from traditional software deployment methods and conclude with a walkthrough of creating docker images and containers.\nWhat is Docker? Docker is an open-source containerization platform (PaaS) that enables users to package applications along with their required dependencies into lightweight, portable containers. It simplifies application deployment and execution, making it a popular choice for modern software delivery.\nWhy Docker? Benefits Efficiency Docker streamlines the deployment process with pre-built images, reducing the complexity of setting up environments from scratch. Containers save time and resources by eliminating the need for running a full operating system for each application. Portability Docker\u0026rsquo;s Platform-agnostic containers provide consistent performance across various environments. Known for the resolution of \u0026ldquo;It works on my machine\u0026rdquo;, containers can run anywhere, not just on a local machine. Virtualization VS Containerization Virtualization is a technique that enables the execution of an entire operating system on a physical host using a hypervisor, such as Hyper-V or VMware. This allows multiple virtual machines to run independently on its own resources while being able to intercommunicate with each other.\nContainerization on the other hand, focuses on packaging applications with their dependencies into containers that share the resources of the host system. Containers are more lightweight than VMs because they don\u0026rsquo;t require a full OS to run. Instead, they run in isolated environments on the host operating system.\nCore Concepts Behind-the-Scenes: How Docker Operates Docker\u0026rsquo;s powerful functionality stems from its core framework, Docker Engine that enables containerization. The combination of docker components allows Docker to pull base images from repositories, create containers, and manage them seamlessly.\nDocker Engine Components Docker Host: Where the host system provides hardware resources to support operations. Docker Daemon (dockerd): Primary component that listens and processes user requests, and also responsible for managing images and containers. Docker CLI: The interface users interact for communicating with the Docker system conveniently. CLI interprets user commands into API requests for Docker Daemon. REST API: A stateless API enabling communication between the CLI and Docker Daemon to process and fulfill user requests. A little more about REST API\u0026hellip; REST API is used for session communication between the user and the Docker Daemon. The Docker Daemon listens for REST API requests via docker.sock or over HTTP(S)/TCP protocols. Since REST APIs are stateless, they do not retain information about previous requests; each request is independent. This stateless nature works because the user includes all necessary information in each request to ensure it can be fulfilled without relying on prior context. Docker Engine Workflow User initiates Docker (run via CLI or program). Docker Daemon (dockerd) functions and listens for REST API requests via local socket or remote TCP socket. User sends requests via CLI, which interprets the commands and forwards them as REST API requests to the Docker Daemon. Docker Daemon processes the request and performs the desired action (e.g., creating a container, displays running containers). User can then interact with Docker resources and experience the Docker system. Cross-Platform Operation Docker Desktop: Enables to run Linux based Docker system on Windows or macOS without additional emulation features. WSL Integration: Integrating Docker Desktop with the WSL (Windows Subsystem for Linux) optimizes performances providing a native-like Linux environment for Windows users. Try Dockerfile Start with base images available online such as Docker Hub Create custom images by writing a Dockerfile script The custom images are used to create \u0026amp; run containers Containers operate in an isolated environment Runs separately from the host system Still relies on the host\u0026rsquo;s resources (CPU, memory, storage, \u0026hellip;) Image Image is a static template used to create a container, which is editable and usable multiple times. Edit Dockerfike if any updates needed for applications, very convenient \u0026amp; efficient way for development and deployment. Container - Port When running a container, you get to configure port numbers. There are 2 port numbers which are used on host and container respectively.\n-p host-port:container-port\nhost-port is opened externally for incoming traffics. container-port is at the door of containers listening to connections. Basically they map each other as you configured at container build. Create a Dockerfile Specify and provide requirements in a Dockerfile Define a desired base image Write required configurations, dependencies, or libraries 1 2 3 4 5 FROM image:tag RUN apt-get update \u0026amp;\u0026amp; apt-get install -y nginx COPY ./webServer RUN ./web.sh CMD ./conf.sh Run the Dockerfile to create a container docker -t image:tag directoryOfDockerfile docker -t webApp:first . -\u0026gt; then Docker installs the listed dependencies or packages Run the container docker run -p 5000:443 --name myContainer webApp:first Dockerfile - Best practice Minimize image size Use base images with the minimalist alpine Linux distribution Install without cache using the flag --no-cache-dir Keep the script clean List a required dependencies in a file (.txt) 1 2 3 4 5 6 FROM python:3.10-alpine WORKDIR /kube-WebApp COPY requirements.txt /kube-WebApp RUN pip install -r dependencies.txt --no-cache-dir COPY . /kube-WebApp CMD python app.py ","date":"2025-02-26T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/docker-basics/docker-banner_hu_b4fe722bbd5d425.png","permalink":"https://rose-jang.github.io/roselogs/p/docker-basics/","title":"Docker Basics"},{"content":"This project demonstrates how to deploy a web application on a Linux-based AWS EC2 instance. While there are several ways to publish a web application, it focuses on using User data to automate the setup. By leveraging user data, we can streamline the configuration process, allowing the instance to perform initial setup tasks automatically during boot.\nPrerequisites AWS account (preferably IAM admin user) Simple HTML code for static website Create AWS EC2 instance Since this project has modest resource requirements, I have selected a minimum-level instance. Despite its low resource allocation, this instance is more than capable of efficiently serving the web application. Default settings are applied during the instance initialization process.\nBasic components AMI: Amazon Linux - maximize efficiency, minimize power resources Type: t2.micro - free tier, perfect for small project Firewall (security group): Create with the default settings Instance summary: Additional configuration Firewall: Configure a security group for public access Add an inbound rule to allow traffic from public users Ensure the HTTP/HTTPS protocols are enabled, as they are essential for initiating and maintaining web services (HTTP is used to support this non-production project) Inbound rule\nType: HTTP provides request-response service on a web browser between client-server Protocol: TCP ensures reliable data transmission Port: 80 port number designated for HTTP Source: 0.0.0.0/0 indicates any IP addresses (all range) Writing a User Data User data is a powerful feature in AWS allows to automate tasks during the initialization of an EC2 instance. By providing a shell script in the User Data field, we can:\nAutomate repetitive setup tasks, reducing manual intervention. Configure and prepare the instance during its boot process, saving time. Ensure the instance is ready to serve its purpose immediately after launch. Below is the shell script that automates the tasks required to run a web server. Insert the script as a file (.sh) or as a text in the box.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # Part 1. System Update \u0026amp; Installation # Ensure the system is up to date and installs web server (Nginx). # After updates, start and enable the Nginx service. #!/bin/bash # Escalate to admin user sudo su # Update and install system package yum update -y yum install nginx # Start the web server systemctl start nginx systemctl enable nginx # Confirm the web service starts on boot chkconfig nginx on # Part 2. Retrieving AWS Metadata # Retrieves AWS instance metadata for live demonstration. # Extract and save instance\u0026#39;s `availability zone` as a variable- AZ. # Retrieve metadata of the instance TOKEN=`curl -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` AZ=`curl -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; http://169.254.169.254/latest/meta-data/placement/availability-zone` # Part 3. Create a Web Page File # Create an `index.html` file as a default landing page. # Create a web page file in .html cat \u0026gt; /usr/share/nginx/html/index.html \u0026lt;\u0026lt;EOF \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;WebApp | Rose\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { color: #f0f8ff; background-color: #809b4c; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div align=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;NGINX Web App\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;This application was deployed from AWS EC2\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Instance Availability Zone: $AZ\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Rose Jang\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; EOF Run the instance \u0026amp; Web server Using the user data script, we pre-configured everything necessary for the web server:\nInstalled and configured the web server Started and enabled the web server Created and saved an index.html file to server as the landing page Now it\u0026rsquo;s time to test the web server by launching the instance and verifying that it is running successfully.\nLaunching the Instance Double-check the instance settings and the shell script correctly saved in user data. Also ensure that the Security Group has an inbound rule allowing http traffic (port 80) to enable public access to the web server.\nLocate the Instance Details Once the instance is running, navigate to the Details section to check its status and availability zone. This will be used to verify if the metadata is correctly passed in the script. Locate the public IP address to access the web server. Ensure the instance is in a \u0026ldquo;Running\u0026rdquo; state and that all health checks have passed before proceeding.\nInstance details\nID: ending with d7d7 Availability Zone: us-east-1c Public IP address: 54.91.44.217 Test the Web Server Access the public IP address in a web browser using http protocol. (ex. http://54.91.44.217) Confirm that the Nginx web server is running and the landing page matches the content of the index.html file.\nTIP! If the web server is not running or there are any issues, check the log file for details. The log file provides detailed information about the execution of the user data script and helps troubleshoot any errors or misconfigurations.\n1 cat /var/log/cloud-init-output.log Conclusion This project successfully demonstrated how to deploy a web application on an AWS EC2 instance using Nginx. Utilizing User data reduces time on initializing core configurations including web server installation, setup, and deployment of a static landing page. This streamlined approach highlights the efficiency and flexibility of cloud infrastructure for hosting a web application.\n","date":"2025-02-02T00:00:00Z","image":"https://rose-jang.github.io/roselogs/p/host-a-web-application-on-aws/cloud-banner_hu_ca3b8fc3ee807614.png","permalink":"https://rose-jang.github.io/roselogs/p/host-a-web-application-on-aws/","title":"Host a Web Application on AWS"}]